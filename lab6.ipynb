{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f2491979c10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/home/yihao/Downloads/spark-2.1.0-bin-hadoop2.6/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " u'supports general computation graphs for data analysis. It also supports a',\n",
       " u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " u'MLlib for machine learning, GraphX for graph processing,',\n",
       " u'and Spark Streaming for stream processing.',\n",
       " u'',\n",
       " u'<http://spark.apache.org/>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#',\n",
       " u'apache',\n",
       " u'spark',\n",
       " u'spark',\n",
       " u'is',\n",
       " u'a',\n",
       " u'fast',\n",
       " u'and',\n",
       " u'general',\n",
       " u'cluster']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x.lower()).flatMap(lambda x: x.split()).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'alternatively,', 1),\n",
       " (u'\"local\"', 1),\n",
       " (u'including', 4),\n",
       " (u'computation', 1),\n",
       " (u'note', 1),\n",
       " (u'submit', 1),\n",
       " (u'using:', 1),\n",
       " (u'guidance', 2),\n",
       " (u'environment', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x.lower(),1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'alternatively,', 1),\n",
       " (u'\"local\"', 1),\n",
       " (u'including', 4),\n",
       " (u'computation', 1),\n",
       " (u'note', 1),\n",
       " (u'submit', 1),\n",
       " (u'using:', 1),\n",
       " (u'guidance', 2),\n",
       " (u'environment', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x.lower(),1)) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda x:sum(x)) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = sc.textFile(\"/home/yihao/Downloads/spark-2.1.0-bin-hadoop2.6/README.md\",minPartitions=3)\n",
    "chunks.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1), (u'alternatively,', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapper(klvls):\n",
    "    for k1v1 in klvls:\n",
    "        for word in k1v1.split():\n",
    "            yield (word.lower(),1)\n",
    "            \n",
    "wc = chunks.mapPartitions(mapper).reduceByKey(lambda x,y:x+y)\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [(u'the', 25), (u'to', 19), (u'spark', 16)])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "def mapper2(counts):\n",
    "    yield (0,heapq.nlargest(3,counts,key=lambda x:x[1]))\n",
    "\n",
    "def reducer2(top31,top32):\n",
    "    return heapq.nlargest(3,top31 + top32, key=lambda x:x[1])\n",
    "\n",
    "top3 = wc.mapPartitions(mapper2).reduceByKey(reducer2)\n",
    "top3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 25), (u'to', 19), (u'spark', 16)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "def mapper2(counts):\n",
    "    yield heapq.nlargest(3,counts,key=lambda x:x[1])\n",
    "    \n",
    "def reducer2(top31,top32):\n",
    "    return heapq.nlargest(3,top31+top32,key=lambda x:x[1])\n",
    "\n",
    "top3 = wc.mapPartitions(mapper2).reduce(reducer2)\n",
    "top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = sc.textFile('citibike.csv').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['801', '379', '2474', '818']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapper3(partId,records):\n",
    "    if partId==0:\n",
    "        records.next()\n",
    "    import csv\n",
    "    reader = csv.reader(records)\n",
    "    for trip in reader:\n",
    "        yield trip[2]\n",
    "        \n",
    "\n",
    "output= trips.mapPartitionsWithIndex(mapper3)\n",
    "output.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
